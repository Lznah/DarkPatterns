\chapter{State of the art}
Most studies \cite{dark-patterns-brignull} \cite{dark-patterns-colin} in the field of dark patterns have only described known existing types of dark patterns. Also, literature often proposes different dark pattern taxonomies. To find these patterns, scholars did manual research, analyzing page by page.

In contrast to this approach, that requires a lot of manual work, there is a study from Princeton University \cite{dark-patterns-at-scale} and it proposes a completely new taxonomy. Not only the researchers recategorized and made more accurate the currently known types from the literature, but they were able to find new types of dark patterns, thus they extended the literature about these new types.

Princeton researchers also notes that only textual information on webshops were analysed. Continues, that the set of found dark pattern is restricted in this manner \cite{dark-patterns-at-scale}.

To find these new types, reseachers focused on product pages of webshops, because as they say, these pages are the most promising to contain dark patterns at any level of purchase flow \cite{dark-patterns-at-scale}. Princeton Researchers did a lot of work to find these dark patterns. They separated it into three steps as it can be seen in figure \ref{pic:dark-patterns.pdf}.

\imagefigurefull{dark-patterns.pdf}{Overview of the shopping website corpus creation, data collection using crawling, and data analysis as proposed by Princeton University researchers\cite{dark-patterns-at-scale}.}{1}

\textbf{Corpus Creation} is the first step, there are several programmes to get domain names of webshops. They gathered websites with the highest Alexa Rank via Alexa Rank API. Then, they used paid service Webshrinker to filter out only those websites that are webshops. The list of domain still contained non-Enlish websites. They used polyglot language classifier to filter them out of the list. Overall, researches gathered a list of 19K English shopping websites\cite{dark-patterns-at-scale}.

\textbf{Data Collection} is the second step. It consist of two crawlers created by the Princeton researches. The first crawler is meant to find product links on a single website. To speed up the process of finding these product pages, they trained a classifier of Logistic Regression on a dataset of 1000 URL links manually labeled by the researchers. The first crawler found 53K pages in 11K domain names.

The second crawler, also refered as 'checkout crawler', is meant to simulate user's shopping flow. This means that the crawler is able to follow the steps of the buying process - which includes selecting product options (e.g., size or color), adding the product to the card, viewing the cart and checking out. To evaluate, whether or not this crawler is able to simulate user's shopping flow, the researchers randomly sampled 100 product pages and examined whether the crawler successfully reached the checkout page.

This crawler is build on OpenWPN, which is a web privacy measurement framework for privacy studies on a large set of websites\cite{github-openwpm}. Princeton researches implemented additional features to this framework. For example, they created a feature to store HAR files, which contain all the HTTP communication and Javascript calls. All these collected data are further utilized in an analysis phase by researchers. These data help researched to recognize whether or not a found pattern is on of the types of dark patterns.

The checkout crawler also divides visited pages into meaningful textual segments. Researchers propose the definition of this textual segment and an algorithm to split the HTML code of the page into these segments \cite{dark-patterns-at-scale}. Also, the checkout crawler extracts data about text and background colors, positions and dimensions of the segments and others. With this algorithm, there were able to capture approximately 13 million segments across the previously noted 53K product URL pages.

\textbf{Data Analysis} is the last step of the research. It consists of data preprocessing, hierarchical clustering, examining and analyzing the found clusters. The data cleansing phase reduced 90\% of all segments to ~1.3 million segments.

Data were transformed into representation of Bag of Words (BoW)\cite{bag-of-words}. Then, Principal Component Analysis was performed on the BoW matrix. The outcome were 3 components, which together represented 95\% of the variance in the data. 

Researchers chose an algorithms called Hierarchical Density-Based Spatial Clustering of Application with Noise (HDB-SCAN)\cite{hdbscan} to find clusters in data. They tried different hyper parameters of this clustering algorithm and picked the most promising results.

Then, they did two passes in examining the clusters. In the first pass, they manually tagged clusters that can manifest as dark patterns. This pass reduced the number of the clusters from 10,277 to 1,768. In pass two, they manually examined all the 1,768 clusters, whether the cluster contains any dark pattern \cite{dark-patterns-at-scale}.

Lastly, the researchers discussed the results and they iteratively grouped the discovered dark patterns into categories. They revealed 15 types of dark patterns in 7 categories on 1,254 websites, which represents ~11,1\% out of 10,277 \cite{dark-patterns-at-scale}.




