\chapter{State of the art}
Most studies\cite{dark-patterns-brignull,dark-patterns-colin,taxonomies-conti} in the field of dark patterns have only described known existing types of dark patterns. Also, literature often proposes different dark pattern taxonomies. To find these patterns, scholars did manual research, analysing page by page.

In contrast to this approach, which requires much manual work, there is a study from Princeton University\cite{dark-patterns-at-scale}, and it proposes an entirely new taxonomy. Not only the researchers recategorised and made more accurate the currently known types from the literature, but they were able to find new types of dark patterns; thus, they extended the literature about these new types.

Princeton researchers also note that only textual information on webshops was analysed. Continues that the set of the found dark patterns is restricted in this manner\cite{dark-patterns-at-scale}.

In an attempt to find these new types, researchers focused on product pages of webshops, because as they say, these pages are the most promising to contain dark patterns at any level of purchase flow\cite{dark-patterns-at-scale}. Princeton Researchers did much work to find these dark patterns. They separated it into three steps, as can be seen in figure \ref{pic:dark-patterns.pdf}.

\imagefigurefull{dark-patterns.pdf}{Overview of the shopping website corpus creation, data collection using crawling, and data analysis, as proposed by Princeton University researchers.\cite{dark-patterns-at-scale}.}{1}

\textbf{Corpus Creation} is the first step; there are several scripts to get domain names of webshops. They gathered websites with the highest Alexa Rank via Alexa Rank API. Then, they used paid service Webshrinker to filter out only those websites that are webshops. The list of domain still contained non-English websites. They used a language classifier library Polyglot to filter them out of the list. Overall, researchers gathered a list of 19K English shopping websites\cite{dark-patterns-at-scale}.

\textbf{Data Collection} is the second step. It consists of two crawlers created by Princeton researchers. The first crawler is meant to find product links on a single website. To speed up the process of finding these product pages, they trained a classifier of Logistic Regression on a dataset of 1000 URL links manually labelled by the researchers. The first crawler found 53K pages in 11K domain names.

The second crawler, also referred to as checkout crawler, is meant to simulate users' shopping flow.  This ability to simulate users' flow means that the crawler follows the buying process steps, including selecting product options (e.g., size or colour), adding the product to the card, viewing the cart, and checking out. To evaluate whether or not this crawler can simulate users' shopping flow, the researchers randomly sampled 100 product pages and examined whether the crawler successfully reached the checkout page.

This crawler is built on OpenWPN, which is a web privacy measurement framework for privacy studies on a large set of websites. Princeton researchers implemented additional features to this framework. For example, they created a feature to store HAR files, which contain all the HTTP communication and Javascript calls. All these collected data are further utilised in an analysis phase by researchers. These data help researched recognise whether or not a found pattern is one of the types of dark patterns.

The checkout crawler also divides visited pages into meaningful textual segments. Researchers define this textual segment and an algorithm to split the page's HTML code into these segments\cite{dark-patterns-at-scale}. Also, the checkout crawler extracts data about text and background colours, positions and dimensions of the segments and others. With this algorithm, they captured approximately 13 million segments across the previously noted 53K product URL pages.

\textbf{Data Analysis} is the last step of the research. It consists of data preprocessing, hierarchical clustering, examining and analysing the found clusters. The data cleansing phase reduced 90\% of all segments to ~1.3 million segments.

Data were transformed into a representation of Bag of Words (BoW)\cite{bag-of-words}. Then, Principal Component Analysis was performed on the BoW matrix. The outcome was three components, which together represented 95\% of the variance in the data. 

Researchers chose an algorithm called Hierarchical Density-Based Spatial Clustering of Application with Noise (HDB-SCAN)\cite{hdbscan} to find clusters in data. They tried different hyperparameters of this clustering algorithm and picked the most promising results.

Then, they did two passes examining the clusters. In the first pass, they manually tagged clusters that can manifest as dark patterns. This pass reduced the number of the clusters from 10,277 to 1,768.  During the second examination, researchers manually examined which of these 1,768 clusters contain dark patterns\cite{dark-patterns-at-scale}.

Lastly, the researchers discussed the results, and they iteratively grouped the discovered dark patterns into categories. They revealed 15 types of dark patterns in 7 categories on 1,254 websites, representing ~11,1\% out of 10,277\cite{dark-patterns-at-scale}.




